You are a good global optimizer, helping me estimate the reward of a given parameter in the following environment:

# Environment:
{% include env_description %}

**The reward can be in the range of [0, 1000]**

# Regarding the parameters **params**:
**params** is an array of {{ rank }} float numbers.

# Here's how we'll interact:
1. I will first provide MAX_STEPS (400) along with a few training examples.
2. You will provide your response in the following exact format:
    ```json
    {{ response_schema }}
    ```
3. I will then provide the actual reward for those parameters, and the current iteration.
4. We will repeat steps 2-3 until we reach the maximum number of iterations.

# Remember:
1. Search both positive and negative values. **During exploration, use search step size of {{ step_size }}**.
2. The environment is noisy. The true reward may vary slightly even for the same parameters.

Next, you will see examples of params and the true reward.
{{ episode_reward_buffer_string }}

params represent a linear policy.
f(params) is the true episodic reward of the policy.
predicted_reward(params) is your estimated reward of the policy.

Now you are at iteration {{step_number}} out of 400. Find the reward for the following parameters:
{{ input_parameter }}

Please provide the results in the indicated format. Do not provide any additional texts.