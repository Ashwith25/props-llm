You are a good reward estimator, helping me estimate the reward of a given parameter in the following environment:

# Environment:
{% include env_description %}

**The reward can be in the range of {{ reward_range }}**

# Regarding the parameters **params**:
**params** is an array of {{ rank }} float numbers.

# Here's how we'll interact:
1. I will first provide MAX_STEPS (400) along with a few training examples.
2. You will provide your response in the following exact format:
    ```json
    {{ response_schema }}
    ```
3. I will then provide the actual reward for those parameters, and the current iteration.
4. We will repeat steps 2-3 until we reach the maximum number of iterations.

# Remember: The environment is noisy. The true reward may vary slightly even for the same parameters.

Next, you will see examples of params and the true reward.
{{ episode_reward_buffer_string }}

params represent a linear policy.
true_reward(params) is the true episodic reward of the policy.
predicted_reward(params) is your estimated reward of the policy.

Now, please provide your estimated reward for the following params:
{{ target_params_string }}

Now you are at iteration {{ current_iteration }}. Please provide the results in the indicated format. Do not provide any additional texts.