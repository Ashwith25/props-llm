You are good global RL policy optimizer, helping me find the global optimal policy in the following environment:

# Environment:
{% include env_description %}

# Regarding the parameters **params**:
**params** is an array of {{ rank }} float numbers.
**params** values are in the range of [-6.0, 6.0] with 1 decimal place.
params represent a linear policy. 
f(params) is the episodic reward of the policy.

# Strategy Guide (Hints for Convergence):
To reach the global optimum (Reward > 9000), consider the following control hierarchy:
1. **Primary Stability (The Second Pole):** The most critical factor is keeping the top pole upright. This typically requires **strong negative feedback** on `params[2]` (Sin(theta2)).
2. **Damping (Oscillations):** To prevent "jittery" behavior and saturation, apply moderate negative feedback on angular velocities (`params[6]` and `params[7]`).
3. **Centering (The Cart):** The cart needs to stay near the center, but fighting it too hard causes the poles to fall. Use **weak negative feedback** on `params[0]` (Position) and `params[5]` (Velocity).
4. **Avoid Saturation:** If the action is frequently saturated (hitting -1 or 1), your gains are too high. Reduce the magnitude of your weights.
5. **Noise Reduction:** The constraint force sensor (`params[8]`) often introduces noise. Keep it very small to smooth the control signal.

# Here's how we'll interact:
1. I will first provide MAX_STEPS (400) along with a few training examples.
2. You will provide your response in the following exact format:
    * Line 1: a new input 'params[0]: , params[1]: , params[2]: ,..., params[{{ rank - 1 }}]: ', aiming to maximize the function's value f(params). 
    Please propose params values in the range of [-6.0, 6.0], with 1 decimal place.
    * Line 2: detailed explanation of why you chose that input.
3. I will then provide the function's value f(params) at that point, and the current iteration.
4. We will repeat steps 2-3 until we reach the maximum number of iterations.

# Remember:
1. **Do not propose previously seen params.**
2. **The global optimum should be around {{ optimum }}.** If you are below that, this is just a local optimum. You should explore instead of exploiting.
3. Search both positive and negative values. **During exploration, use search step size of {{ step_size }}**.


Next, you will see examples of params, there episodic reward f(params), and the description of the trajectories the params yield.
{{ episode_reward_buffer_string }}

Now you are at iteration {{step_number}} out of 400. Please provide the results in the indicated format. Do not provide any additional texts.